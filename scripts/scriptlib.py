from __future__ import annotations
import os, importlib.util, types, json, dotenv, re
from typing import Any, Union, get_origin, get_args
from pydantic import BaseModel
from pathlib import Path
from datetime import datetime, timezone


dotenv.load_dotenv()

# Root of the repository relative to this file
REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Root of the solution using Pathlib
ROOT = Path(__file__).resolve().parent.parent

# Map bare Python types to TS primitives (won't be used for generic types)
PY_TO_TS = {
  str: 'string',
  int: 'number',
  float: 'number',
  bool: 'boolean',
  type(None): 'null',
  dict: 'Record<string, any>',
  datetime: 'string',
  list: 'any[]',  # fallback for untyped list
}

# Standard header included at the top of generated files
HEADER_COMMENT = [
  "// ================================================",
  "// WARNING: This file is automatically generated.",
  "// Do not modify it by hand. Your changes will be",
  "// overwritten the next time the generator runs.",
  "// ================================================",
  "",
]


def camel_case(name: str) -> str:
  """Convert snake_case names to CamelCase."""
  return "".join(part.capitalize() for part in name.split("_"))


def load_module(path: str) -> types.ModuleType:
  module_name = os.path.splitext(os.path.basename(path))[0] + "_tmp"
  spec = importlib.util.spec_from_file_location(module_name, path)
  if not spec or not spec.loader:
    raise ImportError(f"Could not load spec for module at {path}")
  module = importlib.util.module_from_spec(spec)
  spec.loader.exec_module(module)
  return module


def py_to_ts(py_type: Any) -> str:
  origin = get_origin(py_type)
  args = get_args(py_type)

  # Handle generics: List[X] or Tuple[X, ...]
  if origin in (list, tuple):
    inner_type = py_to_ts(args[0]) if args else 'any'
    return f"{inner_type}[]"

  # Handle Optional[X] (i.e. Union[X, None])
  if origin is Union:
    non_none = [arg for arg in args if arg is not type(None)]
    if len(non_none) == 1:
      return f"{py_to_ts(non_none[0])} | null"
    return " | ".join(py_to_ts(arg) for arg in args)

  # Known primitives
  if py_type in PY_TO_TS:
    return PY_TO_TS[py_type]

  # Pydantic models â†’ use interface name
  if isinstance(py_type, type) and issubclass(py_type, BaseModel):
    return py_type.__name__

  # Fallback
  return 'any'


def model_to_ts(model: type[BaseModel]) -> str:
  lines = [f"export interface {model.__name__} {{"]
  fields = getattr(model, 'model_fields', None) or getattr(model, '__fields__', {})
  for name, field in fields.items():
    annotation = getattr(field, 'annotation', None) or getattr(field, 'outer_type_', None)
    ts_type = py_to_ts(annotation)
    lines.append(f"  {name}: {ts_type};")
  lines.append("}")
  return "\n".join(lines)


def parse_version(ver: str) -> tuple[int, int, int, int]:
  """Convert a version string like 'v1.2.3.4' into its numeric parts."""
  ver = ver.lstrip('v')
  major, minor, patch, build = [int(p) for p in ver.split('.')]
  return major, minor, patch, build


def next_build(current_version: str, last_version: str) -> int:
  """Return the next build number for the given versions.

  The build number is reset only when the major or minor version changes.
  Patch version bumps continue the build count within the same minor version.
  """
  cur_major, cur_minor, _, cur_build = parse_version(current_version)
  last_major, last_minor, _, _ = parse_version(last_version)
  if (cur_major, cur_minor) != (last_major, last_minor):
    return 1
  return cur_build + 1


def bump_version(version: str, part: str) -> str:
  """Increment the specified part of a version string.

  Parts can be 'major', 'minor', 'patch', or 'build'.
  Major/minor bumps reset lower-order fields to 0.
  """
  ma, mi, pa, bu = parse_version(version)
  match part:
    case 'major':
      ma += 1
      mi = 0
      pa = 0
      bu = 0
    case 'minor':
      mi += 1
      pa = 0
      bu = 0
    case 'patch':
      pa += 1
    case 'build':
      bu += 1
    case _:
      raise ValueError(f"Unknown part: {part}")
  return f"v{ma}.{mi}.{pa}.{bu}"

async def _fetch_json(cur):
  parts: list[str] = []
  while True:
    row = await cur.fetchone()
    if not row:
      break
    parts.append(row[0])
  return json.loads(''.join(parts)) if parts else []

async def _fetch_dicts(cur):
  rows = await cur.fetchall()
  if not rows:
    return []
  cols = [d[0] for d in cur.description]
  return [dict(zip(cols, row)) for row in rows]

def _quote(name: str) -> str:
  return '[' + name.replace(']', ']]') + ']'

def _qualify(schema: str, name: str) -> str:
  return f"{_quote(schema)}.{_quote(name)}"

async def list_tables(conn):
  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT TABLE_SCHEMA AS table_schema,
                TABLE_NAME AS table_name
           FROM INFORMATION_SCHEMA.TABLES
          WHERE TABLE_TYPE='BASE TABLE'
            AND TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'sys')
          ORDER BY TABLE_SCHEMA, TABLE_NAME"""
    )
    return await _fetch_dicts(cur)

async def list_views(conn):
  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT SCHEMA_NAME(v.schema_id) AS view_schema,
                v.name AS view_name,
                m.definition AS view_definition
           FROM sys.views v
           JOIN sys.sql_modules m ON v.object_id = m.object_id
          WHERE v.is_ms_shipped = 0"""
    )
    return await _fetch_dicts(cur)

async def list_view_dependencies(conn):
  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT SCHEMA_NAME(v.schema_id) AS referencing_schema,
                v.name AS referencing_name,
                SCHEMA_NAME(r.schema_id) AS referenced_schema,
                r.name AS referenced_name
           FROM sys.sql_expression_dependencies d
           JOIN sys.views v ON d.referencing_id = v.object_id
           JOIN sys.views r ON d.referenced_id = r.object_id
          WHERE d.referencing_class_desc='VIEW'
            AND d.referenced_class_desc='VIEW'
            AND v.is_ms_shipped = 0
            AND r.is_ms_shipped = 0"""
    )
    return await _fetch_dicts(cur)

async def list_columns(conn, schema: str, table: str):
  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT c.name AS column_name,
                t.name AS data_type,
                c.max_length,
                c.precision,
                c.scale,
                c.is_nullable,
                dc.definition AS default_definition,
                ic.seed_value,
                ic.increment_value,
                c.is_identity,
                c.is_rowguidcol,
                cc.definition AS computed_definition,
                cc.is_persisted,
                c.collation_name
           FROM sys.columns c
           JOIN sys.types t
             ON c.user_type_id = t.user_type_id
          LEFT JOIN sys.default_constraints dc
             ON c.default_object_id = dc.object_id
          LEFT JOIN sys.identity_columns ic
             ON c.object_id = ic.object_id AND c.column_id = ic.column_id
          LEFT JOIN sys.computed_columns cc
             ON c.object_id = cc.object_id AND c.column_id = cc.column_id
          WHERE c.object_id = OBJECT_ID(?)
          ORDER BY c.column_id""",
      (f"{schema}.{table}",),
    )
    return await _fetch_dicts(cur)

def _group_key_columns(rows: list[dict]) -> list[str]:
  ordered: list[str] = []
  for row in sorted(rows, key=lambda r: r['key_ordinal'] or 0):
    col = _quote(row['column_name'])
    if row.get('is_descending_key'):
      col += ' DESC'
    ordered.append(col)
  return ordered

async def _table_schema(conn, schema: str, table: str):
  raw_columns = await list_columns(conn, schema, table)
  columns: list[dict] = []
  for col in raw_columns:
    columns.append(
      {
        'name': col['column_name'],
        'data_type': col['data_type'],
        'max_length': col['max_length'],
        'precision': col['precision'],
        'scale': col['scale'],
        'nullable': bool(col['is_nullable']),
        'default': col['default_definition'],
        'identity': bool(col['is_identity']),
        'identity_seed': col['seed_value'],
        'identity_increment': col['increment_value'],
        'rowguidcol': bool(col['is_rowguidcol']),
        'computed': col['computed_definition'],
        'computed_persisted': bool(col['is_persisted']),
        'collation': col['collation_name'],
      }
    )

  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT kc.name AS constraint_name,
                i.type_desc,
                ic.key_ordinal,
                c.name AS column_name,
                ic.is_descending_key
           FROM sys.key_constraints kc
           JOIN sys.indexes i
             ON kc.parent_object_id = i.object_id
            AND kc.unique_index_id = i.index_id
           JOIN sys.index_columns ic
             ON kc.parent_object_id = ic.object_id
            AND kc.unique_index_id = ic.index_id
           JOIN sys.columns c
             ON ic.object_id = c.object_id
            AND ic.column_id = c.column_id
          WHERE kc.parent_object_id = OBJECT_ID(?)
            AND kc.type = 'PK'""",
      (f"{schema}.{table}",),
    )
    pk_rows = await _fetch_dicts(cur)

  pk = None
  if pk_rows:
    pk = {
      'name': pk_rows[0]['constraint_name'],
      'type_desc': pk_rows[0]['type_desc'],
      'columns': _group_key_columns(pk_rows),
    }

  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT kc.name AS constraint_name,
                i.type_desc,
                ic.key_ordinal,
                c.name AS column_name,
                ic.is_descending_key
           FROM sys.key_constraints kc
           JOIN sys.indexes i
             ON kc.parent_object_id = i.object_id
            AND kc.unique_index_id = i.index_id
           JOIN sys.index_columns ic
             ON kc.parent_object_id = ic.object_id
            AND kc.unique_index_id = ic.index_id
           JOIN sys.columns c
             ON ic.object_id = c.object_id
            AND ic.column_id = c.column_id
          WHERE kc.parent_object_id = OBJECT_ID(?)
            AND kc.type = 'UQ'""",
      (f"{schema}.{table}",),
    )
    uq_rows = await _fetch_dicts(cur)

  unique_map: dict[str, dict] = {}
  for row in uq_rows:
    entry = unique_map.setdefault(
      row['constraint_name'],
      {
        'name': row['constraint_name'],
        'type_desc': row['type_desc'],
        'columns': [],
      },
    )
    entry['columns'].append(
      (
        row['key_ordinal'] or 0,
        _quote(row['column_name']) + (' DESC' if row.get('is_descending_key') else ''),
      )
    )
  unique_constraints = []
  for entry in sorted(unique_map.values(), key=lambda e: e['name']):
    entry['columns'] = [col for _, col in sorted(entry['columns'])]
    unique_constraints.append(entry)

  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT fk.name AS constraint_name,
                COL_NAME(fkc.parent_object_id, fkc.parent_column_id) AS column_name,
                fkc.constraint_column_id,
                SCHEMA_NAME(ro.schema_id) AS ref_schema,
                ro.name AS ref_table,
                COL_NAME(fkc.referenced_object_id, fkc.referenced_column_id) AS ref_column_name,
                fk.delete_referential_action_desc,
                fk.update_referential_action_desc,
                fk.is_not_for_replication,
                fk.is_not_trusted,
                fk.is_disabled
           FROM sys.foreign_keys fk
           JOIN sys.foreign_key_columns fkc
             ON fk.object_id = fkc.constraint_object_id
           JOIN sys.objects ro
             ON fk.referenced_object_id = ro.object_id
          WHERE fk.parent_object_id = OBJECT_ID(?)
          ORDER BY fk.name, fkc.constraint_column_id""",
      (f"{schema}.{table}",),
    )
    fk_rows = await _fetch_dicts(cur)

  fk_map: dict[str, dict] = {}
  for row in fk_rows:
    entry = fk_map.setdefault(
      row['constraint_name'],
      {
        'name': row['constraint_name'],
        'columns': [],
        'ref_columns': [],
        'ref_schema': row['ref_schema'],
        'ref_table': row['ref_table'],
        'on_delete': row['delete_referential_action_desc'],
        'on_update': row['update_referential_action_desc'],
        'not_for_replication': bool(row['is_not_for_replication']),
        'is_not_trusted': bool(row['is_not_trusted']),
        'is_disabled': bool(row['is_disabled']),
      },
    )
    entry['columns'].append((row['constraint_column_id'], _quote(row['column_name'])))
    entry['ref_columns'].append((row['constraint_column_id'], _quote(row['ref_column_name'])))

  foreign_keys = []
  for entry in sorted(fk_map.values(), key=lambda e: e['name']):
    entry['columns'] = [col for _, col in sorted(entry['columns'])]
    entry['ref_columns'] = [col for _, col in sorted(entry['ref_columns'])]
    foreign_keys.append(entry)

  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT cc.name AS constraint_name,
                cc.definition,
                cc.is_not_trusted,
                cc.is_disabled
           FROM sys.check_constraints cc
          WHERE cc.parent_object_id = OBJECT_ID(?)""",
      (f"{schema}.{table}",),
    )
    raw_checks = await _fetch_dicts(cur)

  check_constraints = [
    {
      'name': row['constraint_name'],
      'definition': row['definition'],
      'is_not_trusted': bool(row['is_not_trusted']),
      'is_disabled': bool(row['is_disabled']),
    }
    for row in raw_checks
  ]

  async with conn.cursor() as cur:
    await cur.execute(
      """SELECT i.name AS index_name,
                i.is_unique,
                i.type_desc,
                i.has_filter,
                i.filter_definition,
                ic.is_included_column,
                ic.key_ordinal,
                ic.index_column_id,
                ic.is_descending_key,
                c.name AS column_name
           FROM sys.indexes i
           JOIN sys.index_columns ic
             ON i.object_id = ic.object_id
            AND i.index_id = ic.index_id
           JOIN sys.columns c
             ON ic.object_id = c.object_id
            AND ic.column_id = c.column_id
          WHERE i.object_id = OBJECT_ID(?)
            AND i.is_primary_key = 0
            AND i.is_unique_constraint = 0
            AND i.[type] <> 0
            AND i.is_hypothetical = 0
            AND i.name IS NOT NULL
          ORDER BY i.name,
                   CASE WHEN ic.is_included_column = 0 THEN ic.key_ordinal ELSE ic.index_column_id END""",
      (f"{schema}.{table}",),
    )
    index_rows = await _fetch_dicts(cur)

  index_map: dict[str, dict] = {}
  for row in index_rows:
    entry = index_map.setdefault(
      row['index_name'],
      {
        'name': row['index_name'],
        'is_unique': bool(row['is_unique']),
        'type_desc': row['type_desc'],
        'has_filter': bool(row['has_filter']),
        'filter_definition': row['filter_definition'],
        'key_columns': [],
        'included_columns': [],
      },
    )
    target = entry['included_columns'] if row['is_included_column'] else entry['key_columns']
    col = _quote(row['column_name'])
    if row['is_descending_key'] and not row['is_included_column']:
      col += ' DESC'
    order = row['index_column_id'] if row['is_included_column'] else row['key_ordinal']
    target.append((order or 0, col))

  indexes = []
  for entry in sorted(index_map.values(), key=lambda e: e['name']):
    entry['key_columns'] = [col for _, col in sorted(entry['key_columns'])]
    entry['included_columns'] = [col for _, col in sorted(entry['included_columns'])]
    indexes.append(entry)

  return {
    'schema': schema,
    'name': table,
    'columns': columns,
    'primary_key': pk,
    'unique_constraints': unique_constraints,
    'foreign_keys': foreign_keys,
    'check_constraints': check_constraints,
    'indexes': indexes,
  }

async def get_schema(conn):
  tables = await list_tables(conn)
  schemas: dict[str, dict] = {}
  deps: dict[str, set[str]] = {}
  for t in tables:
    schema = t['table_schema']
    name = t['table_name']
    key = f"{schema}.{name}"
    info = await _table_schema(conn, schema, name)
    schemas[key] = info
    deps[key] = {
      f"{fk['ref_schema']}.{fk['ref_table']}" for fk in info['foreign_keys']
    }
  ordered: list[str] = []
  visited: set[str] = set()

  def visit(n: str):
    if n in visited:
      return
    visited.add(n)
    for dep in deps.get(n, set()):
      if dep in deps:
        visit(dep)
    ordered.append(n)

  for t in deps.keys():
    visit(t)

  view_defs = await list_views(conn)
  view_map = {
    f"{v['view_schema']}.{v['view_name']}": v for v in view_defs
  }
  view_deps: dict[str, set[str]] = {}
  for d in await list_view_dependencies(conn):
    src = f"{d['referencing_schema']}.{d['referencing_name']}"
    dst = f"{d['referenced_schema']}.{d['referenced_name']}"
    view_deps.setdefault(src, set()).add(dst)
  vordered: list[str] = []
  vvisited: set[str] = set()

  def vvisit(n: str):
    if n in vvisited:
      return
    vvisited.add(n)
    for dep in view_deps.get(n, set()):
      if dep in view_deps:
        vvisit(dep)
    vordered.append(n)

  for key in view_map.keys():
    vvisit(key)

  return {
    'tables': [schemas[n] for n in ordered],
    'views': [
      {
        'schema': view_map[n]['view_schema'],
        'name': view_map[n]['view_name'],
        'definition': view_map[n]['view_definition'],
      }
      for n in vordered
      if n in view_map
    ],
  }

def _format_data_type(col: dict) -> str:
  dtype = col['data_type']
  if not dtype:
    return ''
  upper = dtype.upper()
  type_name = upper if dtype.islower() else dtype
  length = col['max_length']
  precision = col['precision']
  scale = col['scale']

  if upper in {'CHAR', 'VARCHAR', 'BINARY', 'VARBINARY'}:
    if length == -1:
      return f"{type_name}(MAX)"
    if length is not None:
      return f"{type_name}({length})"
    return type_name

  if upper in {'NCHAR', 'NVARCHAR'}:
    if length == -1:
      return f"{type_name}(MAX)"
    if length is not None:
      chars = length // 2 if length else 0
      return f"{type_name}({chars})"
    return type_name

  if upper in {'DECIMAL', 'NUMERIC'} and precision is not None and scale is not None:
    return f"{type_name}({precision},{scale})"

  if upper in {'TIME', 'DATETIME2', 'DATETIMEOFFSET'} and scale is not None:
    return f"{type_name}({scale})"

  if upper == 'FLOAT' and precision not in {None, 53}:
    return f"{type_name}({precision})"

  return type_name

def _format_column(col: dict) -> str:
  name = _quote(col['name'])
  if col['computed']:
    line = f"{name} AS {col['computed']}"
    if col['computed_persisted']:
      line += ' PERSISTED'
    return line

  parts = [name, _format_data_type(col)]
  if col['collation']:
    parts.append(f"COLLATE {col['collation']}")
  if col['identity']:
    seed = col['identity_seed'] if col['identity_seed'] is not None else 1
    inc = col['identity_increment'] if col['identity_increment'] is not None else 1
    parts.append(f"IDENTITY({seed}, {inc})")
  if col['rowguidcol']:
    parts.append('ROWGUIDCOL')
  parts.append('NULL' if col['nullable'] else 'NOT NULL')
  if col['default']:
    parts.append(f"DEFAULT {col['default']}")
  return ' '.join(parts)

def _build_create_sql(table: dict) -> str:
  table_name = _qualify(table['schema'], table['name'])
  column_lines = [_format_column(col) for col in table['columns']]
  constraints: list[str] = []
  pk = table.get('primary_key')
  if pk:
    type_desc = pk['type_desc'].replace('_', ' ') if pk['type_desc'] else ''
    clause = f"CONSTRAINT {_quote(pk['name'])} PRIMARY KEY"
    if type_desc:
      clause += f" {type_desc}"
    clause += f" ({', '.join(pk['columns'])})"
    constraints.append(clause)
  body = ',\n  '.join(column_lines + constraints)
  return f"CREATE TABLE {table_name} (\n  {body}\n);"

def _build_unique_constraint_sql(table: dict, constraint: dict) -> str:
  table_name = _qualify(table['schema'], table['name'])
  type_desc = constraint['type_desc'].replace('_', ' ') if constraint['type_desc'] else ''
  clause = f"ALTER TABLE {table_name} ADD CONSTRAINT {_quote(constraint['name'])} UNIQUE"
  if type_desc:
    clause += f" {type_desc}"
  clause += f" ({', '.join(constraint['columns'])});"
  return clause

def _build_check_constraint_sql(table: dict, constraint: dict) -> list[str]:
  table_name = _qualify(table['schema'], table['name'])
  name = _quote(constraint['name'])
  prefix = 'WITH NOCHECK ' if constraint['is_not_trusted'] else ''
  statements = [
    f"ALTER TABLE {table_name} {prefix}ADD CONSTRAINT {name} CHECK {constraint['definition']};"
  ]
  if constraint['is_disabled']:
    statements.append(f"ALTER TABLE {table_name} NOCHECK CONSTRAINT {name};")
  return statements

def _build_index_sql(table: dict, index: dict) -> str:
  table_name = _qualify(table['schema'], table['name'])
  parts = ['CREATE']
  if index['is_unique']:
    parts.append('UNIQUE')
  type_desc = index['type_desc'].replace('_', ' ') if index['type_desc'] else ''
  if type_desc:
    parts.append(type_desc)
  parts.append('INDEX')
  parts.append(_quote(index['name']))
  parts.append('ON')
  parts.append(table_name)
  if index['key_columns']:
    parts.append(f"({', '.join(index['key_columns'])})")
  if index['included_columns']:
    parts.append(f"INCLUDE ({', '.join(index['included_columns'])})")
  if index['has_filter'] and index['filter_definition']:
    parts.append(f"WHERE {index['filter_definition']}")
  return ' '.join(parts) + ';'

def _build_foreign_key_sql(table: dict, fk: dict) -> list[str]:
  table_name = _qualify(table['schema'], table['name'])
  ref_table = _qualify(fk['ref_schema'], fk['ref_table'])
  name = _quote(fk['name'])
  prefix = 'WITH NOCHECK' if fk['is_not_trusted'] or fk['is_disabled'] else 'WITH CHECK'
  statement = (
    f"ALTER TABLE {table_name} {prefix} ADD CONSTRAINT {name} FOREIGN KEY "
    f"({', '.join(fk['columns'])}) REFERENCES {ref_table} ({', '.join(fk['ref_columns'])})"
  )
  if fk['not_for_replication']:
    statement += ' NOT FOR REPLICATION'
  if fk['on_delete'] and fk['on_delete'] != 'NO_ACTION':
    statement += f" ON DELETE {fk['on_delete'].replace('_', ' ')}"
  if fk['on_update'] and fk['on_update'] != 'NO_ACTION':
    statement += f" ON UPDATE {fk['on_update'].replace('_', ' ')}"
  statements = [statement + ';']
  if fk['is_disabled']:
    statements.append(f"ALTER TABLE {table_name} NOCHECK CONSTRAINT {name};")
  return statements

def _normalize_view_definition(view: dict) -> str:
  raw_def = re.sub(r'--.*?(\r?\n|$)', ' ', view['definition'])
  definition = ' '.join(raw_def.split())
  target_name = f"{_qualify(view['schema'], view['name'])}"
  if not definition.lower().startswith('create'):
    definition = f"CREATE VIEW {target_name} AS {definition}"
  if not definition.upper().startswith('CREATE VIEW'):
    definition = f"CREATE VIEW {target_name} AS {definition}"  # fallback
  if not definition.endswith(';'):
    definition += ';'
  return definition

async def dump_schema(conn, prefix: str = 'schema') -> str:
  schema = await get_schema(conn)
  ts = datetime.now(timezone.utc).strftime('%Y%m%d')
  filename = f"{prefix}_{ts}.sql"

  table_stmts: list[str] = []
  unique_stmts: list[str] = []
  check_stmts: list[str] = []
  index_stmts: list[str] = []
  fk_stmts: list[str] = []

  for table in schema['tables']:
    table_stmts.append(_build_create_sql(table))
    for constraint in table['unique_constraints']:
      unique_stmts.append(_build_unique_constraint_sql(table, constraint))
    for constraint in table['check_constraints']:
      check_stmts.extend(_build_check_constraint_sql(table, constraint))
    for index in table['indexes']:
      index_stmts.append(_build_index_sql(table, index))
    for fk in table['foreign_keys']:
      fk_stmts.extend(_build_foreign_key_sql(table, fk))

  view_stmts: list[str] = [
    _normalize_view_definition(view) for view in schema.get('views', [])
  ]

  sections = [
    table_stmts,
    unique_stmts,
    check_stmts,
    index_stmts,
    fk_stmts,
    view_stmts,
  ]

  lines: list[str] = ['SET ANSI_NULLS ON;', 'SET QUOTED_IDENTIFIER ON;', '']
  for stmts in sections:
    if not stmts:
      continue
    if lines and lines[-1] != '':
      lines.append('')
    lines.extend(stmts)
  with open(filename, 'w') as f:
    f.write('\n'.join(line for line in lines if line is not None))
  print(f'Schema dumped to {filename}')
  return filename

async def dump_data(conn, prefix: str = 'dump_data') -> str:
  schema = await get_schema(conn)
  data: dict[str, list[dict]] = {}
  for tbl in schema['tables']:
    table_name = _qualify(tbl['schema'], tbl['name'])
    key = f"{tbl['schema']}.{tbl['name']}"
    async with conn.cursor() as cur:
      await cur.execute(f"SELECT * FROM {table_name} FOR JSON PATH")
      data[key] = await _fetch_json(cur)
  ts = datetime.now(timezone.utc).strftime('%Y%m%d_BACKUP')
  filename = f"{prefix}_{ts}.json"
  with open(filename, 'w') as f:
    json.dump({'schema': schema, 'data': data}, f, indent=2, default=str)
  print(f'Data dumped to {filename}')
  return filename

async def apply_schema(conn, path: str):
  with open(path, 'r') as f:
    sql = f.read()
  async with conn.cursor() as cur:
    for stmt in sql.split(';'):
      stmt = stmt.strip()
      if not stmt:
        continue
      await cur.execute(stmt)
  print('Schema applied.')

async def connect(dbname=None):
  try:
    import aioodbc  # type: ignore
  except Exception as e:
    raise ImportError('aioodbc is required for database operations') from e
  dsn = os.getenv('AZURE_SQL_CONNECTION_STRING')
  if not dsn:
    raise RuntimeError('AZURE_SQL_CONNECTION_STRING not set')
  if dbname:
    parts = []
    replaced = False
    for part in dsn.split(';'):
      if part.upper().startswith('DATABASE=') or part.upper().startswith('INITIAL CATALOG='):
        parts.append(f'DATABASE={dbname}')
        replaced = True
      else:
        parts.append(part)
    if not replaced:
      parts.append(f'DATABASE={dbname}')
    dsn = ';'.join(parts)
  conn = await aioodbc.connect(dsn=dsn, autocommit=True)
  print(f'Connected to database {dbname or dsn}')
  return conn
